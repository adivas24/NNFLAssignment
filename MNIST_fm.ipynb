{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_fm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaKjO/9+2MyPy0wEcU4tOK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adivas24/NNFLAssignment/blob/main/MNIST_fm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKzZZAKnRPiv",
        "outputId": "6110a8fc-7113-4f90-c565-bed99cebd5e4"
      },
      "source": [
        "!git clone https://github.com/Sleepychord/ImprovedGAN-pytorch.git\n",
        "%cd /content/ImprovedGAN-pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ImprovedGAN-pytorch'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Total 77 (delta 0), reused 0 (delta 0), pack-reused 77\u001b[K\n",
            "Unpacking objects: 100% (77/77), done.\n",
            "/content/ImprovedGAN-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SClbeN1LRkiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48ce9ec-7f03-4e41-dd07-ae0ba5894c82"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "import pdb\n",
        "import math\n",
        "from __future__ import print_function \n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "!pip3 install tensorboardX\n",
        "import tensorboardX"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPr0_OvqRtcn"
      },
      "source": [
        "def log_sum_exp(x, axis = 1):\n",
        "    return torch.max(x, dim = 1)[0] + torch.log(torch.sum(torch.exp(x - m.unsqueeze(1)), dim = axis))\n",
        "\n",
        "def rnp(L, stdv, weight_scale = 1.):\n",
        "    assert type(L) == torch.nn.Linear\n",
        "    torch.nn.init.normal(L.weight, std=weight_scale / math.sqrt(L.weight.size()[0]))\n",
        "    \n",
        "class CustomLayer(torch.nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True, weight_scale=None, weight_init_stdv=0.1):\n",
        "        super(CustomLayer, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.randn(out_features, in_features) * weight_init_stdv)\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.zeros(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        if weight_scale is not None:\n",
        "            assert type(weight_scale) == int\n",
        "            self.weight_scale = Parameter(torch.ones(out_features, 1) * weight_scale)\n",
        "        else:\n",
        "            self.weight_scale = 1 \n",
        "    \n",
        "    def forward(self, x):\n",
        "        W = self.weight * self.weight_scale / torch.sqrt(torch.sum(self.weight ** 2, dim = 1, keepdim = True))\n",
        "        return F.linear(x, W, self.bias)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ7oDfRxRseW"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim = 28*28, output_dim = 10):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            CustomLayer(input_dim, 1000),\n",
        "            CustomLayer(1000, 500),\n",
        "            CustomLayer(500, 250),\n",
        "            CustomLayer(250, 250),\n",
        "            CustomLayer(250, 250)]\n",
        "        )\n",
        "        self.final = CustomLayer(250, output_dim, weight_scale=1)\n",
        "\n",
        "    def forward(self, x, feature_matching = False, cuda = False):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        noise = torch.randn(x.size()) * 0.3 if self.training else torch.Tensor([0])\n",
        "        if cuda:\n",
        "            noise = noise.cuda()\n",
        "        x = x + Variable(noise, requires_grad = False)\n",
        "        for i in range(len(self.layers)):\n",
        "            m = self.layers[i]\n",
        "            x_f = F.relu(m(x))\n",
        "            noise = torch.randn(x_f.size()) * 0.5 if self.training else torch.Tensor([0])\n",
        "            if cuda:\n",
        "                noise = noise.cuda()\n",
        "            x = (x_f + Variable(noise, requires_grad = False))\n",
        "        if feature_matching:\n",
        "            return x_f, self.final(x)\n",
        "        return self.final(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, output_dim = 28 ** 2):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.layerfc1 = nn.Linear(z_dim, 500, bias = False)\n",
        "        self.layerbn1 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
        "        self.layerfc2 = nn.Linear(500, 500, bias = False)\n",
        "        self.layerbn2 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
        "        self.layerfc3 = CustomLayer(500, output_dim, weight_scale = 1)\n",
        "        self.bn1_b , self.bn2_b = Parameter(torch.zeros(500)), Parameter(torch.zeros(500))\n",
        "        nn.init.xavier_uniform(self.layerfc1.weight)\n",
        "        nn.init.xavier_uniform(self.layerfc2.weight)\n",
        "\n",
        "    def forward(self, batch_size, cuda = False):\n",
        "        x = Variable(torch.rand(batch_size, self.z_dim), requires_grad = False, volatile = not self.training)\n",
        "        x = F.softplus(self.layerbn1(self.layerfc1(x)) + self.bn1_b)\n",
        "        x = F.softplus(self.layerbn2(self.layerfc2(x)) + self.bn2_b)\n",
        "        x = F.softplus(self.layerfc3(x))\n",
        "        return x\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7F-eHz3R3Yl"
      },
      "source": [
        "class GAN(object):\n",
        "    def __init__(self, G, D, lab, unlab, test):\n",
        "        self.G ,self.D, self.labeled, self.unlabeled, self.test  = G, D, lab, unlab, test\n",
        "        self.writer = tensorboardX.SummaryWriter(log_dir='./logfile')\n",
        "        # using ADAM optimizer\n",
        "        self.Doptim = optim.Adam(self.D.parameters(), lr=0.003, betas= (0.5, 0.999))\n",
        "        self.Goptim = optim.Adam(self.G.parameters(), lr=0.003, betas = (0.5,0.999))\n",
        "\n",
        "    def trainDiscriminator(self, x_label, y, x_unlabel):\n",
        "        x_label, x_unlabel, y = Variable(x_label), Variable(x_unlabel), Variable(y, requires_grad = False)\n",
        "        \n",
        "        label_out=self.D(x_label, cuda=False)\n",
        "        unlabel_out=self.D(x_unlabel, cuda=False)\n",
        "        fake_out = self.D(self.G(x_unlabel.size()[0], cuda = False).view(x_unlabel.size()).detach(), cuda=False)\n",
        "\n",
        "        ls = -torch.mean(torch.gather(label_out, 1, y.unsqueeze(1))) + torch.mean(log_sum_exp(label_out))\n",
        "        lu = 0.5 * (-torch.mean(log_sum_exp(unlabel_out)) + torch.mean(F.softplus(log_sum_exp(unlabel_out)))+ torch.mean(F.softplus(log_sum_exp(fake_out))) )\n",
        "        loss = ls + lu\n",
        "        #print(loss)\n",
        "        self.Doptim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.Doptim.step()\n",
        "        return ls.data.cpu().numpy(), lu.data.cpu().numpy(), torch.mean((label_out.max(1)[1] == y).float())\n",
        "    \n",
        "    def trainGenerator(self, x_unlabel):\n",
        "        # FEATURE MATCHING\n",
        "        generated, fake = self.D(self.G(x_unlabel.size()[0], cuda = False).view(x_unlabel.size()), feature_matching=True, cuda=False)\n",
        "        unlabel, _ = self.D(Variable(x_unlabel), feature_matching=True, cuda=False)\n",
        "\n",
        "        generated= torch.mean(generated, dim = 0)\n",
        "        unlabel = torch.mean(unlabel, dim = 0)\n",
        "        loss_feature_matching = torch.mean((generated - unlabel) ** 2)\n",
        "        self.Goptim.zero_grad()\n",
        "        self.Doptim.zero_grad()\n",
        "        loss_feature_matching.backward()\n",
        "        self.Goptim.step()\n",
        "        return loss_feature_matching.data.cpu().numpy()\n",
        "\n",
        "    def train(self):\n",
        "        #assert self.unlabeled.__len__() > self.labeled.__len__()\n",
        "        #assert type(self.labeled) == TensorDataset\n",
        "        times = int(np.ceil(self.unlabeled.__len__() * 1. / self.labeled.__len__()))\n",
        "        t1 = self.labeled.tensors[0].clone()\n",
        "        t2 = self.labeled.tensors[1].clone()\n",
        "        tile_labeled = TensorDataset(t1.repeat(times,1,1,1),t2.repeat(times))\n",
        "        gn = 0\n",
        "        for epoch in range(10):\n",
        "            self.G.train()\n",
        "            self.D.train()\n",
        "            unlabel_loader1 = DataLoader(self.unlabeled, batch_size = 100, shuffle=True, drop_last=True, num_workers = 4)\n",
        "            unlabel_loader2 = DataLoader(self.unlabeled, batch_size = 100, shuffle=True, drop_last=True, num_workers = 4).__iter__()\n",
        "            label_loader = DataLoader(tile_labeled, batch_size = 100, shuffle=True, drop_last=True, num_workers = 4).__iter__()\n",
        "            loss_supervised = loss_unsupervised = loss_gen = accuracy = 0.\n",
        "            batch_num = 0\n",
        "            for (unlabel1, _label1) in unlabel_loader1:\n",
        "                batch_num += 1\n",
        "                unlabel2, _label2 = unlabel_loader2.next()\n",
        "                x, y = label_loader.next()\n",
        "                ll, lu, acc = self.trainDiscriminator(x, y, unlabel1)\n",
        "                \n",
        "                loss_supervised += ll\n",
        "                loss_unsupervised += lu\n",
        "                accuracy += acc\n",
        "                lg = self.trainGenerator(unlabel2)\n",
        "                if epoch > 1 and lg > 1:\n",
        "                    lg = self.trainGenerator(unlabel2)\n",
        "                loss_gen += lg\n",
        "                if (batch_num + 1) % 100 == 0:\n",
        "                    print('Training: %d / %d' % (batch_num + 1, len(unlabel_loader1)))\n",
        "                    gn += 1\n",
        "                    with torch.no_grad():\n",
        "                        self.writer.add_scalars('loss', {'loss_supervised':ll, 'loss_unsupervised':lu, 'loss_gen':lg}, gn)\n",
        "                        self.writer.add_histogram('real_feature', self.D(Variable(x), cuda=False, feature_matching = True)[0], gn)\n",
        "                        self.writer.add_histogram('fake_feature', self.D(self.G(100, cuda = False), cuda=False, feature_matching = True)[0], gn)\n",
        "                        self.writer.add_histogram('fc3_bias', self.G.layerfc3.bias, gn)\n",
        "                        self.writer.add_histogram('D_feature_weight', self.D.layers[-1].weight, gn)\n",
        "                    self.D.train()\n",
        "                    self.G.train()\n",
        "            loss_supervised /= batch_num\n",
        "            loss_unsupervised /= batch_num\n",
        "            loss_gen /= batch_num\n",
        "            accuracy /= batch_num\n",
        "            print(\"Iteration %d, loss_supervised = %.4f, loss_unsupervised = %.4f, loss_gen = %.4f train acc = %.4f\" % (epoch, loss_supervised, loss_unsupervised, loss_gen, accuracy))\n",
        "            sys.stdout.flush()\n",
        "            if (epoch + 1) % 1 == 0:\n",
        "                print(\"Eval: correct %d / %d\"  % (self.eval(), self.test.__len__()))\n",
        "                \n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            pred = torch.max(self.D(Variable(x), cuda=False), 1)[1].data\n",
        "        return pred\n",
        "\n",
        "    def eval(self):\n",
        "        self.G.eval()\n",
        "        self.D.eval()\n",
        "        d, l = [], []\n",
        "        for (datum, label) in self.test:\n",
        "            d.append(datum)\n",
        "            l.append(label)\n",
        "        x, y = torch.stack(d), torch.LongTensor(l)\n",
        "        pred = self.predict(x)\n",
        "        return torch.sum(pred == y)\n",
        "    def draw(self, batch_size):\n",
        "        self.G.eval()\n",
        "        return self.G(batch_size, cuda=False)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTeeb2joR79J",
        "outputId": "d5bd8aff-f15a-4139-e337-396f4f5129f2"
      },
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "def get_labelled_MNIST(class_num):\n",
        "    raw_dataset = datasets.MNIST('../data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),]))\n",
        "    class_total, data , labels, positive_total, total = 10*[0] , [], [], 0 ,0\n",
        "    perm = np.random.permutation(raw_dataset.__len__())\n",
        "    for i in range(raw_dataset.__len__()):\n",
        "        datum, label = raw_dataset.__getitem__(perm[i])\n",
        "        if class_total[label] < class_num:\n",
        "            data.append(datum.numpy())\n",
        "            labels.append(label)\n",
        "            class_total[label] += 1\n",
        "            total += 1\n",
        "            if total >= 10*class_num:\n",
        "                break\n",
        "    return TensorDataset(torch.FloatTensor(np.array(data)), torch.LongTensor(np.array(labels)))\n",
        "\n",
        "MNIST_labelled=get_labelled_MNIST(10)\n",
        "MNIST_unlabelled=raw_dataset = datasets.MNIST('../data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),]))\n",
        "MNIST_test=datasets.MNIST('../data', train=False, download=True,transform=transforms.Compose([transforms.ToTensor(),]))\n",
        "\n",
        "model = GAN(Generator(100), Discriminator(), MNIST_labelled, MNIST_unlabelled, MNIST_test)\n",
        "model.train()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training: 100 / 600\n",
            "Training: 200 / 600\n",
            "Training: 300 / 600\n",
            "Training: 400 / 600\n",
            "Training: 500 / 600\n",
            "Training: 600 / 600\n",
            "Iteration 0, loss_supervised = 0.1515, loss_unsupervised = 0.4526, loss_gen = 0.1472 train acc = 0.9483\n",
            "Eval: correct 8016 / 10000\n",
            "Training: 100 / 600\n",
            "Training: 200 / 600\n",
            "Training: 300 / 600\n",
            "Training: 400 / 600\n",
            "Training: 500 / 600\n",
            "Training: 600 / 600\n",
            "Iteration 1, loss_supervised = 0.0092, loss_unsupervised = 0.3951, loss_gen = 0.2532 train acc = 0.9983\n",
            "Eval: correct 8620 / 10000\n",
            "Training: 100 / 600\n",
            "Training: 200 / 600\n",
            "Training: 300 / 600\n",
            "Training: 400 / 600\n",
            "Training: 500 / 600\n",
            "Training: 600 / 600\n",
            "Iteration 2, loss_supervised = 0.0066, loss_unsupervised = 0.3806, loss_gen = 0.3698 train acc = 0.9989\n",
            "Eval: correct 8757 / 10000\n",
            "Training: 100 / 600\n",
            "Training: 200 / 600\n",
            "Training: 300 / 600\n",
            "Training: 400 / 600\n",
            "Training: 500 / 600\n",
            "Training: 600 / 600\n",
            "Iteration 3, loss_supervised = 0.0056, loss_unsupervised = 0.3771, loss_gen = 0.5710 train acc = 0.9991\n",
            "Eval: correct 8976 / 10000\n",
            "Training: 100 / 600\n",
            "Training: 200 / 600\n",
            "Training: 300 / 600\n",
            "Training: 400 / 600\n",
            "Training: 500 / 600\n",
            "Training: 600 / 600\n",
            "Iteration 4, loss_supervised = 0.0042, loss_unsupervised = 0.3773, loss_gen = 0.7348 train acc = 0.9994\n",
            "Eval: correct 9051 / 10000\n",
            "Training: 100 / 600\n",
            "Training: 200 / 600\n",
            "Training: 300 / 600\n",
            "Training: 400 / 600\n",
            "Training: 500 / 600\n",
            "Training: 600 / 600\n",
            "Iteration 5, loss_supervised = 0.0033, loss_unsupervised = 0.3827, loss_gen = 0.8137 train acc = 0.9996\n",
            "Eval: correct 9302 / 10000\n",
            "Training: 100 / 600\n",
            "Training: 200 / 600\n",
            "Training: 300 / 600\n",
            "Training: 400 / 600\n",
            "Training: 500 / 600\n",
            "Training: 600 / 600\n",
            "Iteration 6, loss_supervised = 0.0031, loss_unsupervised = 0.3893, loss_gen = 0.8601 train acc = 0.9996\n",
            "Eval: correct 9387 / 10000\n",
            "Training: 100 / 600\n",
            "Training: 200 / 600\n",
            "Training: 300 / 600\n",
            "Training: 400 / 600\n",
            "Training: 500 / 600\n",
            "Training: 600 / 600\n",
            "Iteration 7, loss_supervised = 0.0026, loss_unsupervised = 0.3935, loss_gen = 0.9349 train acc = 0.9997\n",
            "Eval: correct 9428 / 10000\n",
            "Training: 100 / 600\n",
            "Training: 200 / 600\n",
            "Training: 300 / 600\n",
            "Training: 400 / 600\n",
            "Training: 500 / 600\n",
            "Training: 600 / 600\n",
            "Iteration 8, loss_supervised = 0.0023, loss_unsupervised = 0.3975, loss_gen = 0.9916 train acc = 0.9998\n",
            "Eval: correct 9570 / 10000\n",
            "Training: 100 / 600\n",
            "Training: 200 / 600\n",
            "Training: 300 / 600\n",
            "Training: 400 / 600\n",
            "Training: 500 / 600\n",
            "Training: 600 / 600\n",
            "Iteration 9, loss_supervised = 0.0021, loss_unsupervised = 0.3996, loss_gen = 1.0509 train acc = 0.9998\n",
            "Eval: correct 9549 / 10000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
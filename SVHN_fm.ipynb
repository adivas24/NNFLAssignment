{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x):\n",
    "    m = torch.max(x,1)[0]\n",
    "    s = torch.logsumexp(x-m.unsqueeze(1) , 1)\n",
    "    return s+m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions for loading SVHN dataset\n",
    "\n",
    "def maybe_download(data_dir):\n",
    "    new_data_dir = os.path.join(data_dir, 'svhn')\n",
    "    if not os.path.exists(new_data_dir):\n",
    "        os.makedirs(new_data_dir)\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %.1f%%' % (float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve('http://ufldl.stanford.edu/housenumbers/train_32x32.mat', new_data_dir+'/train_32x32.mat', _progress)\n",
    "        filepath, _ = urllib.request.urlretrieve('http://ufldl.stanford.edu/housenumbers/test_32x32.mat', new_data_dir+'/test_32x32.mat', _progress)\n",
    "\n",
    "def svhn_data_load(data_dir, subset='train'):\n",
    "    maybe_download(data_dir)\n",
    "    if subset=='train':\n",
    "        train_data = loadmat(os.path.join(data_dir, 'svhn') + '/train_32x32.mat')\n",
    "        trainx = train_data['X']\n",
    "        trainy = train_data['y'].flatten()\n",
    "        trainy[trainy==10] = 0\n",
    "        return trainx, trainy\n",
    "    elif subset=='test':\n",
    "        test_data = loadmat(os.path.join(data_dir, 'svhn') + '/test_32x32.mat')\n",
    "        testx = test_data['X']\n",
    "        testy = test_data['y'].flatten()\n",
    "        testy[testy==10] = 0\n",
    "        return testx, testy\n",
    "    else:\n",
    "        raise NotImplementedError('subset should be either train or test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "This section is for reproducability of results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx, trainy = svhn_data_load('data','train')\n",
    "testx, testy = svhn_data_load('data','test')\n",
    "\n",
    "#Remove this once the training happens properly, and GPU's can be used effectively. \n",
    "\n",
    "#trainx = trainx[:,:,:,:10000]\n",
    "#trainy = trainy[:10000]\n",
    "\n",
    "#testx= testx[:,:,:,:10000]\n",
    "#testy = testy[:10000]\n",
    "\n",
    "trainx = np.transpose(trainx,(3,2,0,1))\n",
    "testx = np.transpose(testx,(3,2,0,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dataset class and the data loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SvhnDataset(Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        super(SvhnDataset, self).__init__()\n",
    "        \n",
    "        self.x = x \n",
    "        self.y = y\n",
    "        self.unl1 = x[rng_data.permutation(x.shape[0])]\n",
    "        self.unl2 = x[rng_data.permutation(x.shape[0])]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x_lab = convert_image_dtype(torch.tensor(self.x[idx]))\n",
    "        lab = torch.tensor(self.y[idx],dtype = torch.int64)\n",
    "        x_ul1 = convert_image_dtype(torch.tensor(self.unl1[idx]))\n",
    "        x_ul2 = convert_image_dtype(torch.tensor(self.unl2[idx]))\n",
    "        \n",
    "        return x_lab, lab, x_ul1, x_ul2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, *shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, input):\n",
    "        bs = input.size(0)\n",
    "        return input.view(bs,*self.shape)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential( \n",
    "            nn.Linear(100, 4*4*512), \n",
    "            nn.ReLU(), \n",
    "            nn.BatchNorm1d(4*4*512), \n",
    "            \n",
    "            View(512,4,4), \n",
    "            # state size = 512x4x4\n",
    "            \n",
    "            nn.ConvTranspose2d(512,256,4,2,1), \n",
    "            nn.BatchNorm2d(256), \n",
    "            nn.ReLU(), \n",
    "            # state size = 256x8x8\n",
    "            \n",
    "            nn.ConvTranspose2d(256,128,4,2,1), \n",
    "            nn.BatchNorm2d(128), \n",
    "            nn.ReLU(),\n",
    "            #state size = 128x16x16\n",
    "            \n",
    "            nn.utils.weight_norm(nn.ConvTranspose2d(128,3,4,2,1)),\n",
    "            nn.Tanh()\n",
    "            #state size = 3x32x32\n",
    "        )\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return self.main(input)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just testing the model to see if the output is coming as expected. Also, print a summary of the layers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_noise = torch.rand(32,100)\n",
    "\n",
    "test_gen = Generator()\n",
    "sample_out =  test_gen(sample_noise)\n",
    "\n",
    "print(\"Model Summary:\")\n",
    "print(test_gen,'\\n')\n",
    "print(f'Shape of generator output is {sample_out.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential( \n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.utils.weight_norm(nn.Conv2d(3,64,3,padding=1)), \n",
    "            nn.LeakyReLU(), \n",
    "        \n",
    "            nn.utils.weight_norm(nn.Conv2d(64,64,3,padding=1)), \n",
    "            nn.LeakyReLU(0.2), \n",
    "            \n",
    "            nn.utils.weight_norm(nn.Conv2d(64,64,3,stride=2,padding=1)), \n",
    "            nn.LeakyReLU(0.2), \n",
    "            \n",
    "            nn.Dropout2d(p=0.5), \n",
    "            \n",
    "            nn.utils.weight_norm(nn.Conv2d(64,128,3,padding=1)), \n",
    "            nn.LeakyReLU(0.2), \n",
    "        \n",
    "            nn.utils.weight_norm(nn.Conv2d(128,128,3,padding=1)), \n",
    "            nn.LeakyReLU(0.2), \n",
    "            \n",
    "            nn.utils.weight_norm(nn.Conv2d(128,128,3,stride=2,padding=1)), \n",
    "            nn.LeakyReLU(0.2), \n",
    "            \n",
    "            nn.Dropout2d(p=0.5), \n",
    "            \n",
    "            nn.utils.weight_norm(nn.Conv2d(128,128,3,padding=0)),\n",
    "            nn.LeakyReLU(0.2), \n",
    "            \n",
    "            nn.utils.weight_norm(nn.Conv2d(128,128,1)), \n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.utils.weight_norm(nn.Conv2d(128,128,1)), \n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.AvgPool2d(6),\n",
    "            View(128))\n",
    "        \n",
    "        self.last_layer = nn.utils.weight_norm(nn.Linear(128,10))\n",
    "        \n",
    "    def forward(self,input,feature=False):\n",
    "        if feature:\n",
    "            return self.main(input)\n",
    "        else:\n",
    "            return self.last_layer(self.main(input))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the discriminator with a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_noise = torch.rand(64,3,32,32)\n",
    "test_disc = Discriminator()\n",
    "\n",
    "disc_out = test_disc(sample_noise)\n",
    "disc_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight initialisation function, as copied from the DCGAN tutorial on PyTorch. Remember to change the init values according to the original code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.05)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.05)\n",
    "        nn.init.constant_(m.bias.data, 0)       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the weight initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()\n",
    "disc = Discriminator()\n",
    "\n",
    "gen.apply(weights_init)\n",
    "disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dstep(x_lab, lab, x_ul, noise):\n",
    "\n",
    "    output_before_softmax_lab = disc(x_lab)\n",
    "    output_before_softmax_unl = disc(x_ul)\n",
    "    output_before_softmax_gen = disc(gen(noise))\n",
    "    \n",
    "    l_lab = output_before_softmax_lab[(torch.arange(min(batch_size,lab.shape[0])),lab)]\n",
    "    \n",
    "    l_unl = log_sum_exp(output_before_softmax_unl)\n",
    "    l_gen = log_sum_exp(output_before_softmax_gen)\n",
    "    \n",
    "    loss_lab = -torch.mean(l_lab) + torch.mean(log_sum_exp(output_before_softmax_lab))\n",
    "    loss_unl = -0.5*torch.mean(l_unl) + 0.5*torch.mean(nn.functional.softplus(l_unl)) + 0.5*torch.mean(nn.functional.softplus(l_gen))\n",
    "    train_err = torch.mean(torch.ne(torch.argmax(output_before_softmax_lab,dim=1),lab).float())\n",
    "    \n",
    "    total_loss = loss_lab+ unlabelled_weight*loss_unl\n",
    "    \n",
    "    Doptim.zero_grad()\n",
    "    total_loss.backward()\n",
    "    Doptim.step()\n",
    "    \n",
    "    return loss_lab.item(), loss_unl.item(), train_err.item()\n",
    "\n",
    "\n",
    "def Gstep(x_ul,noise):\n",
    "    \n",
    "    output_unl = disc(x_ul, feature=True)\n",
    "    output_gen = disc(gen(noise), feature=True)\n",
    "\n",
    "    m1 = torch.mean(output_unl,axis=0)\n",
    "    m2 = torch.mean(output_gen,axis=0)\n",
    "    loss_gen = torch.mean(abs(m1-m2))\n",
    "\n",
    "    Goptim.zero_grad()\n",
    "    loss_gen.backward()\n",
    "    Goptim.step()\n",
    "    \n",
    "    return loss_gen.item()\n",
    "\n",
    "\n",
    "def test(x_lab,lab):\n",
    "    \n",
    "    output_before_softmax_lab = disc(x_lab)\n",
    "    train_err = torch.mean(torch.ne(torch.argmax(output_before_softmax_lab,dim=1),lab).float())\n",
    "    \n",
    "    return train_err.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAN(disc,gen,Doptim,Goptim,\n",
    "             dataloader_train,dataloader_test,noise_dim,\n",
    "             lr, num_epochs=1, device = 'cuda'):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        lr = lr*min(3-epoch/400,1)\n",
    "        for param_group in Doptim.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        for param_group in Goptim.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        begin = time.time()\n",
    "        disc.train()\n",
    "        gen.train()\n",
    "\n",
    "        disc.to(device)\n",
    "        gen.to(device)\n",
    "        \n",
    "        loss_lab = 0\n",
    "        loss_unl = 0\n",
    "        train_err = 0\n",
    "        loss_gen = 0\n",
    "        disc_avg_params = None\n",
    "\n",
    "        for _, (x_lab,lab,x_ul1,x_ul2) in enumerate(tqdm.tqdm(dataloader_train, desc=\"Training GAN\")):  \n",
    "\n",
    "            noise = torch.rand(noise_dim)\n",
    "\n",
    "            noise = noise.to(device)\n",
    "            x_lab = x_lab.to(device)\n",
    "            lab = lab.to(device)\n",
    "            x_ul1 = x_ul1.to(device)\n",
    "            x_ul2 = x_ul2.to(device)\n",
    "\n",
    "            ll,lu,te = Dstep(x_lab, lab, x_ul1, noise)\n",
    "\n",
    "            loss_lab+=ll\n",
    "            loss_unl+=lu\n",
    "            train_err+=te\n",
    "\n",
    "            ## Historical averaging\n",
    "\n",
    "            current_param = [param.data for param in disc.parameters()]\n",
    "            if disc_avg_params == None:\n",
    "                disc_avg_params = current_param\n",
    "            else:\n",
    "                disc_avg_params = [a+0.0001*(p-a) for (p,a) in zip(current_param,disc_avg_params)]\n",
    "                \n",
    "            noise = torch.rand(noise_dim).cuda()\n",
    "\n",
    "            lg = Gstep(x_ul2,noise)\n",
    "            loss_gen+=lg\n",
    "\n",
    "        loss_lab/= (_+1)\n",
    "        loss_unl/= (_+1)\n",
    "        train_err/= (_+1)\n",
    "        loss_gen/=(_+1)\n",
    "\n",
    "\n",
    "        # Shifting the averaged parameters to test the model \n",
    "        orig_param = [param.data for param in disc.parameters()]\n",
    "        for param,avg in zip(disc.parameters(),disc_avg_params):\n",
    "            param.data.copy_(avg)\n",
    "\n",
    "        ## Testing the model \n",
    "\n",
    "        test_err = 0\n",
    "        for _,(tx,lab,tx_ul1,tx_ul2) in enumerate(tqdm.tqdm(dataloader_test,desc = \" Testing GAN\")):\n",
    "\n",
    "            disc.eval()\n",
    "            gen.eval()\n",
    "\n",
    "            tx = tx.cuda()\n",
    "            lab = lab.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                te= test(tx,lab)\n",
    "            test_err+=te\n",
    "\n",
    "        test_err/=(_+1)\n",
    "\n",
    "        #Shifting the original parameters back to the model \n",
    "        for param,orig in zip(disc.parameters(),orig_param):\n",
    "            param.data.copy_(orig)\n",
    "\n",
    "        print(f\"Epoch {epoch}, loss_lab = {loss_lab}, loss_unl = {loss_unl}, train err = {train_err}, test err = {test_err}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model \n",
    "\n",
    "This section deals with the training of the model. All dataloaders and models are expected to be instanatiated here, for ease of tuning hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All data related variables \n",
    "\n",
    "seed_data = 1\n",
    "seed = 1\n",
    "count = 50\n",
    "batch_size = 24\n",
    "unlabelled_weight = 1\n",
    "\n",
    "rng_data = np.random.RandomState(seed_data)\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "noise_dim = (batch_size,100)\n",
    "\n",
    "dataloader_train = DataLoader(SvhnDataset(trainx, trainy), batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "dataloader_test = DataLoader(SvhnDataset(testx, testy), batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model related variables\n",
    "\n",
    "disc = Discriminator()\n",
    "gen = Generator()\n",
    "\n",
    "disc.apply(weights_init)\n",
    "gen.apply(weights_init)\n",
    "\n",
    "lr = 0.001\n",
    "betas = (0.9,0.999)\n",
    "\n",
    "Goptim = torch.optim.Adam(gen.parameters(), lr,betas)\n",
    "Doptim = torch.optim.Adam(disc.parameters(),lr,betas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commencing training\n",
    "\n",
    "trainGAN(disc,gen,Doptim,Goptim,\n",
    "         dataloader_train,dataloader_test,noise_dim,\n",
    "         lr, num_epochs=1, device = 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

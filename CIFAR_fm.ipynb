{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_fm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUe31nxbFSMA"
      },
      "source": [
        "Plotting functions. Copied direcly from the code online. Added the line to make it work on notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLAXa9_ypdSR"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "# Plot image examples.\n",
        "def plot_img(img, title):\n",
        "    plt.figure()\n",
        "    plt.imshow(img, interpolation='nearest')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "def img_stretch(img):\n",
        "    img = img.astype(float)\n",
        "    img -= np.min(img)\n",
        "    img /= np.max(img)+1e-12\n",
        "    return img\n",
        "\n",
        "def img_tile(imgs, aspect_ratio=1.0, tile_shape=None, border=1,\n",
        "             border_color=0, stretch=False):\n",
        "    ''' Tile images in a grid.\n",
        "    If tile_shape is provided only as many images as specified in tile_shape\n",
        "    will be included in the output.\n",
        "    '''\n",
        "    # Prepare images\n",
        "    if stretch:\n",
        "        imgs = img_stretch(imgs)\n",
        "    imgs = np.array(imgs)\n",
        "    if imgs.ndim != 3 and imgs.ndim != 4:\n",
        "        raise ValueError('imgs has wrong number of dimensions.')\n",
        "    n_imgs = imgs.shape[0]\n",
        "    # Grid shape\n",
        "    img_shape = np.array(imgs.shape[1:3])\n",
        "    if tile_shape is None:\n",
        "        img_aspect_ratio = img_shape[1] / float(img_shape[0])\n",
        "        aspect_ratio *= img_aspect_ratio\n",
        "        tile_height = int(np.ceil(np.sqrt(n_imgs * aspect_ratio)))\n",
        "        tile_width = int(np.ceil(np.sqrt(n_imgs / aspect_ratio)))\n",
        "        grid_shape = np.array((tile_height, tile_width))\n",
        "    else:\n",
        "        assert len(tile_shape) == 2\n",
        "        grid_shape = np.array(tile_shape)\n",
        "    # Tile image shape\n",
        "    tile_img_shape = np.array(imgs.shape[1:])\n",
        "    tile_img_shape[:2] = (img_shape[:2] + border) * grid_shape[:2] - border\n",
        "    # Assemble tile image\n",
        "    tile_img = np.empty(tile_img_shape)\n",
        "    tile_img[:] = border_color\n",
        "    for i in range(grid_shape[0]):\n",
        "        for j in range(grid_shape[1]):\n",
        "            img_idx = j + i*grid_shape[1]\n",
        "            if img_idx >= n_imgs:\n",
        "                # No more images - stop filling out the grid.\n",
        "                break\n",
        "            img = imgs[img_idx]\n",
        "            yoff = (img_shape[0] + border) * i\n",
        "            xoff = (img_shape[1] + border) * j\n",
        "            tile_img[yoff:yoff+img_shape[0], xoff:xoff+img_shape[1], ...] = img\n",
        "    return tile_img\n",
        "\n",
        "def conv_filter_tile(filters):\n",
        "    n_filters, n_channels, height, width = filters.shape\n",
        "    tile_shape = None\n",
        "    if n_channels == 3:\n",
        "        # Interpret 3 color channels as RGB\n",
        "        filters = np.transpose(filters, (0, 2, 3, 1))\n",
        "    else:\n",
        "        # Organize tile such that each row corresponds to a filter and the\n",
        "        # columns are the filter channels\n",
        "        tile_shape = (n_channels, n_filters)\n",
        "        filters = np.transpose(filters, (1, 0, 2, 3))\n",
        "        filters = np.resize(filters, (n_filters*n_channels, height, width))\n",
        "    filters = img_stretch(filters)\n",
        "    return img_tile(filters, tile_shape=tile_shape)\n",
        "    \n",
        "def scale_to_unit_interval(ndar, eps=1e-8):\n",
        "  \"\"\" Scales all values in the ndarray ndar to be between 0 and 1 \"\"\"\n",
        "  ndar = ndar.copy()\n",
        "  ndar -= ndar.min()\n",
        "  ndar *= 1.0 / (ndar.max() + eps)\n",
        "  return ndar\n",
        "\n",
        "\n",
        "def tile_raster_images(X, img_shape, tile_shape, tile_spacing=(0, 0),\n",
        "                       scale_rows_to_unit_interval=True,\n",
        "                       output_pixel_vals=True):\n",
        "  \"\"\"\n",
        "  Transform an array with one flattened image per row, into an array in\n",
        "  which images are reshaped and layed out like tiles on a floor.\n",
        "\n",
        "  This function is useful for visualizing datasets whose rows are images,\n",
        "  and also columns of matrices for transforming those rows\n",
        "  (such as the first layer of a neural net).\n",
        "\n",
        "  :type X: a 2-D ndarray or a tuple of 4 channels, elements of which can\n",
        "  be 2-D ndarrays or None;\n",
        "  :param X: a 2-D array in which every row is a flattened image.\n",
        "\n",
        "  :type img_shape: tuple; (height, width)\n",
        "  :param img_shape: the original shape of each image\n",
        "\n",
        "  :type tile_shape: tuple; (rows, cols)\n",
        "  :param tile_shape: the number of images to tile (rows, cols)\n",
        "\n",
        "  :param output_pixel_vals: if output should be pixel values (i.e. int8\n",
        "  values) or floats\n",
        "\n",
        "  :param scale_rows_to_unit_interval: if the values need to be scaled before\n",
        "  being plotted to [0,1] or not\n",
        "\n",
        "\n",
        "  :returns: array suitable for viewing as an image.\n",
        "  (See:`PIL.Image.fromarray`.)\n",
        "  :rtype: a 2-d array with same dtype as X.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  assert len(img_shape) == 2\n",
        "  assert len(tile_shape) == 2\n",
        "  assert len(tile_spacing) == 2\n",
        "\n",
        "  # The expression below can be re-written in a more C style as\n",
        "  # follows :\n",
        "  #\n",
        "  # out_shape = [0,0]\n",
        "  # out_shape[0] = (img_shape[0] + tile_spacing[0]) * tile_shape[0] -\n",
        "  #                tile_spacing[0]\n",
        "  # out_shape[1] = (img_shape[1] + tile_spacing[1]) * tile_shape[1] -\n",
        "  #                tile_spacing[1]\n",
        "  out_shape = [(ishp + tsp) * tshp - tsp for ishp, tshp, tsp\n",
        "                      in zip(img_shape, tile_shape, tile_spacing)]\n",
        "\n",
        "  if isinstance(X, tuple):\n",
        "      assert len(X) == 4\n",
        "      # Create an output numpy ndarray to store the image\n",
        "      if output_pixel_vals:\n",
        "          out_array = np.zeros((out_shape[0], out_shape[1], 4), dtype='uint8')\n",
        "      else:\n",
        "          out_array = np.zeros((out_shape[0], out_shape[1], 4), dtype=X.dtype)\n",
        "\n",
        "      #colors default to 0, alpha defaults to 1 (opaque)\n",
        "      if output_pixel_vals:\n",
        "          channel_defaults = [0, 0, 0, 255]\n",
        "      else:\n",
        "          channel_defaults = [0., 0., 0., 1.]\n",
        "\n",
        "      for i in range(4):\n",
        "          if X[i] is None:\n",
        "              # if channel is None, fill it with zeros of the correct\n",
        "              # dtype\n",
        "              out_array[:, :, i] = np.zeros(out_shape,\n",
        "                      dtype='uint8' if output_pixel_vals else out_array.dtype\n",
        "                      ) + channel_defaults[i]\n",
        "          else:\n",
        "              # use a recurrent call to compute the channel and store it\n",
        "              # in the output\n",
        "              out_array[:, :, i] = tile_raster_images(X[i], img_shape, tile_shape, tile_spacing, scale_rows_to_unit_interval, output_pixel_vals)\n",
        "      return out_array\n",
        "\n",
        "  else:\n",
        "      # if we are dealing with only one channel\n",
        "      H, W = img_shape\n",
        "      Hs, Ws = tile_spacing\n",
        "\n",
        "      # generate a matrix to store the output\n",
        "      out_array = np.zeros(out_shape, dtype='uint8' if output_pixel_vals else X.dtype)\n",
        "\n",
        "\n",
        "      for tile_row in range(tile_shape[0]):\n",
        "          for tile_col in range(tile_shape[1]):\n",
        "              if tile_row * tile_shape[1] + tile_col < X.shape[0]:\n",
        "                  if scale_rows_to_unit_interval:\n",
        "                      # if we should scale values to be between 0 and 1\n",
        "                      # do this by calling the `scale_to_unit_interval`\n",
        "                      # function\n",
        "                      this_img = scale_to_unit_interval(X[tile_row * tile_shape[1] + tile_col].reshape(img_shape))\n",
        "                  else:\n",
        "                      this_img = X[tile_row * tile_shape[1] + tile_col].reshape(img_shape)\n",
        "                  # add the slice to the corresponding position in the\n",
        "                  # output array\n",
        "                  out_array[\n",
        "                      tile_row * (H+Hs): tile_row * (H + Hs) + H,\n",
        "                      tile_col * (W+Ws): tile_col * (W + Ws) + W\n",
        "                      ] \\\n",
        "                      = this_img * (255 if output_pixel_vals else 1)\n",
        "      return out_array\n"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udYW8KYi4fGv"
      },
      "source": [
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import pickle\n",
        "import torch\n",
        "from google.colab import files\n",
        "import torch.nn as nn"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBDIYWJ4Fnqd"
      },
      "source": [
        "The upload function allows me to get local data onto colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "BpOB8TFuNtif",
        "outputId": "09c5cdd6-b20e-4b38-966d-67e37148b6f9"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e57af329-4d1c-4c3b-8d80-ea16d7dbe87e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e57af329-4d1c-4c3b-8d80-ea16d7dbe87e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_sample_small to data_sample_small\n",
            "Saving test_batch_small to test_batch_small\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVg1HpIVF02M"
      },
      "source": [
        "Unpickle has been taken directly from original code. Is not needed for smaller test cases, as I preprocessed them locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOIKH2hrSIgL"
      },
      "source": [
        "def unpickle(file):\n",
        "    fo = open(file, 'rb')\n",
        "    d = pickle.load(fo, encoding='latin1')\n",
        "    fo.close()\n",
        "    return {'x': np.cast[np.float32]((-127.5 + d['data'].reshape((10000,3,32,32)))/128.), 'y': np.array(d['labels']).astype(np.uint8)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bqhmoA-F_Gl"
      },
      "source": [
        "Load the data and store it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEHmHc4BSlPh"
      },
      "source": [
        "f1 = open(\"data_sample_small\",\"rb\")\n",
        "f2 = open(\"test_batch_small\",\"rb\")\n",
        "tr,test = pickle.load(f1),pickle.load(f2)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRARAO5oGCLj"
      },
      "source": [
        "Set some parameters that are used later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUS7i4CG8meU"
      },
      "source": [
        "seed = 1\n",
        "seed_data = 1\n",
        "batch_size = 10\n",
        "lr = 0.0003\n",
        "count=50"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ntsq2NqGZM0"
      },
      "source": [
        "Setting seeds and creating random objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCJETlL_5o6j"
      },
      "source": [
        "# fixed random seeds\n",
        "rng_data = np.random.RandomState(seed_data)\n",
        "rng = np.random.RandomState(seed)\n",
        "theano_rng = np.random.RandomState(seed_data+seed)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdaix9LNGdFr"
      },
      "source": [
        "Divying up the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q86hZLlr5lsk"
      },
      "source": [
        "trainx,trainy = tr['x'],tr['y']\n",
        "testx,testy = test['x'],test['y']\n",
        "trainx_unl = trainx.copy()\n",
        "trainx_unl2 = trainx.copy()\n",
        "nr_batches_train = trainx.shape[0]//batch_size\n",
        "nr_batches_test = testx.shape[0]//batch_size"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rfwOsm7GrBe"
      },
      "source": [
        "The generator and discriminator models. They have been designed based on the theano implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WRg-VrbgqPi"
      },
      "source": [
        "# Generator Code\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.l1 = nn.Linear(100, 4*4*512, bias=True)\n",
        "        nn.init.normal_(self.l1.weight, mean=0.0, std=0.05)\n",
        "        self.l2 = nn.BatchNorm1d(4*4*512)\n",
        "        #Change the view here to 512 x 4 x 4\n",
        "        self.l3 = nn.ConvTranspose2d(512, 256, 5, stride =2, padding=2, output_padding=1) # should result in 256 x 8 x 8.\n",
        "        nn.init.normal_(self.l3.weight, mean=0.0, std=0.05)\n",
        "        self.l4 = nn.BatchNorm2d(256)\n",
        "        self.l5 = nn.ConvTranspose2d(256, 128, 5, stride =2, padding=2, output_padding=1) # should result in 128 x 16 x 16.\n",
        "        nn.init.normal_(self.l5.weight, mean=0.0, std=0.05)\n",
        "        self.l6 = nn.BatchNorm2d(128)\n",
        "        self.l7 = nn.utils.weight_norm(nn.ConvTranspose2d(128, 3, 5, stride =2, padding=2, output_padding=1))  # should result in 3 x 32 x 32.\n",
        "        nn.init.normal_(self.l7.weight, mean=0.0, std=0.05)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.l1(input)\n",
        "        x = self.l2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = x.view(x.size(0),512,4,4)\n",
        "        x = self.l3(x)\n",
        "        x = self.l4(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.l5(x)\n",
        "        x = self.l6(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.l7(x)\n",
        "        x = torch.tanh(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.l1 = nn.utils.weight_norm(nn.Conv2d(3, 96, 3, stride = 1, padding=1))\n",
        "        self.l2 = nn.utils.weight_norm(nn.Conv2d(96, 96, 3, stride = 1, padding=1))\n",
        "        self.l3 = nn.utils.weight_norm(nn.Conv2d(96, 96, 3, stride = 2, padding=1))\n",
        "        self.l4 = nn.utils.weight_norm(nn.Conv2d(96, 192, 3, stride=1, padding=1))\n",
        "        self.l5 = nn.utils.weight_norm(nn.Conv2d(192, 192, 3, stride=1, padding=1))\n",
        "        self.l6 = nn.utils.weight_norm(nn.Conv2d(192, 192, 3, stride=2, padding=1))\n",
        "        self.l7 = nn.utils.weight_norm(nn.Conv2d(192, 192, 3, stride=1, padding=0))\n",
        "        self.l8 = nn.utils.weight_norm(nn.Conv2d(192, 192, 1, stride=1, padding=0))\n",
        "        self.l9 = nn.utils.weight_norm(nn.Conv2d(192, 192, 1, stride=1, padding=0))\n",
        "        self.l10 = nn.MaxPool2d(6)\n",
        "        self.l11 = nn.utils.weight_norm(nn.Linear(192, 10, bias=True))\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = nn.functional.dropout(input, p=0.2)\n",
        "        x = self.l1(x)\n",
        "        x = nn.functional.leaky_relu(x, negative_slope=0.2)\n",
        "        x = self.l2(x)\n",
        "        x = nn.functional.leaky_relu(x, negative_slope=0.2)\n",
        "        x = self.l3(x)\n",
        "        x = nn.functional.leaky_relu(x, negative_slope=0.2)\n",
        "        x = nn.functional.dropout(x, p=0.5)\n",
        "        x = self.l4(x)\n",
        "        x = nn.functional.leaky_relu(x, negative_slope=0.2)\n",
        "        x = self.l5(x)\n",
        "        x = nn.functional.leaky_relu(x, negative_slope=0.2)\n",
        "        x = self.l6(x)\n",
        "        x = nn.functional.leaky_relu(x, negative_slope=0.2)\n",
        "        x = nn.functional.dropout(x, p=0.5)\n",
        "        x = self.l7(x)\n",
        "        x = nn.functional.leaky_relu(x, negative_slope=0.2)\n",
        "        x = self.l8(x)\n",
        "        x = nn.functional.leaky_relu(x, negative_slope=0.2)\n",
        "        x = self.l9(x)\n",
        "        x = nn.functional.leaky_relu(x, negative_slope=0.2)\n",
        "        x = self.l10(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = self.l11(x)\n",
        "        return x"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az7CR56BIpm-"
      },
      "source": [
        "This hook allows for feature matching at the second to last layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQo26r9Vv-Cl"
      },
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3fPDsTAJHbC"
      },
      "source": [
        "noise_dim = (batch_size, 100)\n",
        "noise = theano_rng.uniform(size=noise_dim)\n",
        "gen = Generator(0)\n",
        "disc = Discriminator(0)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zGmlOnUwTWK",
        "outputId": "dde55eaf-041b-4651-e7c5-c6c7a32befe1"
      },
      "source": [
        "\n",
        "disc.l10.register_forward_hook(get_activation('l10'))\n"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x7f0f1487f5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwC5h4fEIyIr"
      },
      "source": [
        "Weight initializations are from the original implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meqCLbpNkCrw"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname=m.__class__.__name__\n",
        "    if classname.find('Conv') != -1 or classname.find('ConvTranspose2d')!= -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.05)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.05)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "disc.apply(weights_init)\n",
        "gen.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcqUyGOzJHvs"
      },
      "source": [
        "Data selection is exactly as the original paper. The optimiser used is Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG1agdpU5FnM"
      },
      "source": [
        "# select labeled data\n",
        "inds = rng_data.permutation(trainx.shape[0])\n",
        "trainx = trainx[inds]\n",
        "trainy = trainy[inds]\n",
        "txs = []\n",
        "tys = []\n",
        "for j in range(10):\n",
        "    txs.append(trainx[trainy==j][:count])\n",
        "    tys.append(trainy[trainy==j][:count])\n",
        "txs = np.concatenate(txs, axis=0)\n",
        "tys = np.concatenate(tys, axis=0)\n",
        "\n",
        "# initialize optimisers\n",
        "Doptim = torch.optim.Adam(disc.parameters(),lr=lr, betas = (0.9,0.999))\n",
        "Goptim = torch.optim.Adam(gen.parameters(),lr=lr, betas = (0.9,0.999))"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Ig8gUIJWpm"
      },
      "source": [
        "Training is made parallel to the original implementation. Theano functions were opened up and replaced by their actual implementations in each place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBzqCGBVxreC"
      },
      "source": [
        "for epoch in range(1200):\n",
        "    begin = time.time()\n",
        "    lr = lr*min(3-epoch/400,1)\n",
        "    for param_group in Doptim.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    for param_group in Goptim.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    # This entire process needs to be replaced by a dataloader.\n",
        "    trainx = []\n",
        "    trainy = []\n",
        "    for t in range(int(np.ceil(trainx_unl.shape[0]/float(txs.shape[0])))):\n",
        "        inds = rng.permutation(txs.shape[0])\n",
        "        trainx.append(txs[inds])\n",
        "        trainy.append(tys[inds])\n",
        "    trainx = np.concatenate(trainx, axis=0)\n",
        "    trainy = np.concatenate(trainy, axis=0)\n",
        "    trainx_unl = trainx_unl[rng.permutation(trainx_unl.shape[0])]\n",
        "    trainx_unl2 = trainx_unl2[rng.permutation(trainx_unl2.shape[0])]\n",
        "    \n",
        "    # train\n",
        "    loss_lab = 0.\n",
        "    loss_unl = 0.\n",
        "    train_err = 0.\n",
        "    disc_avg_updates = None\n",
        "    for t in range(nr_batches_train):\n",
        "        ran_from = t*batch_size\n",
        "        ran_to = (t+1)*batch_size\n",
        "        x_lab,labels,x_unl,lr = torch.tensor(trainx[ran_from:ran_to]),torch.tensor(trainy[ran_from:ran_to]),torch.tensor(trainx_unl[ran_from:ran_to]),torch.tensor(lr)\n",
        "        noise = torch.tensor(theano_rng.uniform(size=noise_dim))\n",
        "        disc.train()\n",
        "        gen.train()\n",
        "        output_before_softmax_lab = disc(x_lab)\n",
        "        output_before_softmax_unl = disc(x_unl)\n",
        "        output_before_softmax_gen = disc(gen(noise.float()))\n",
        "        l_lab_ind = [(i,j.item()) for i,j in zip(np.arange(batch_size),labels)]\n",
        "        l_lab = torch.tensor([output_before_softmax_lab[x] for x in l_lab_ind])\n",
        "        l_unl = torch.logsumexp(output_before_softmax_unl,1)\n",
        "        l_gen = torch.logsumexp(output_before_softmax_gen,1)\n",
        "        loss_lab = -torch.mean(l_lab) + torch.mean(torch.mean(torch.logsumexp(output_before_softmax_lab,1)))\n",
        "        loss_unl = -0.5*torch.mean(l_unl) + 0.5*torch.mean(nn.functional.softplus(l_unl)) + 0.5*torch.mean(nn.functional.softplus(l_gen))\n",
        "        total_loss = loss_lab+loss_unl\n",
        "        train_err = torch.mean(torch.ne(torch.argmax(output_before_softmax_lab,dim=1),labels).float())\n",
        "        Doptim.zero_grad()\n",
        "        total_loss.backward()\n",
        "        Doptim.step()\n",
        "\n",
        "        ll, lu, te = loss_lab, loss_unl, train_err\n",
        "        loss_lab += ll\n",
        "        loss_unl += lu\n",
        "        train_err += te\n",
        "\n",
        "        disc_params = [param.data for param in disc.parameters()]\n",
        "        if disc_avg_updates == None:\n",
        "            disc_avg_updates = [p for p in disc_params]\n",
        "        disc_avg_updates = [a+0.0001*(p-a) for p,a in zip(disc_params,disc_avg_updates)]\n",
        "  \n",
        "        noise = torch.tensor(theano_rng.uniform(size=noise_dim))\n",
        "        x_unl,lr = trainx_unl2[t*batch_size:(t+1)*batch_size],lr\n",
        "        disc.train()\n",
        "        gen.train()\n",
        "        output_unl = disc(torch.tensor(x_unl))\n",
        "        # The below one needs to be used for feature mapping, but the loss function and optimiser will need to be redesigned for that.\n",
        "        #output_unl = activation['l10']\n",
        "        output_gen = disc(gen(torch.tensor(noise).float()))\n",
        "        #output_gen = activation['l10']\n",
        "        m1 = torch.mean(output_unl,dim=0)\n",
        "        m2 = torch.mean(output_gen,dim=0)\n",
        "        loss_gen = torch.mean(torch.abs(m1-m2)) # feature matching loss\n",
        "        Goptim.zero_grad()\n",
        "        Doptim.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        Goptim.step()\n",
        "\n",
        "    loss_lab /= nr_batches_train\n",
        "    loss_unl /= nr_batches_train\n",
        "    train_err /= nr_batches_train\n",
        "    # test\n",
        "    gen.eval()\n",
        "    disc.eval()\n",
        "    original_params = [param.data for param in disc.parameters()]\n",
        "    for param,avg_param in zip(disc.parameters(),disc_avg_updates):\n",
        "        param.data.copy_(avg_param)\n",
        "    test_err = 0.\n",
        "    for t in range(nr_batches_test):\n",
        "        x_lab,labels = testx[t*batch_size:(t+1)*batch_size],testy[t*batch_size:(t+1)*batch_size]\n",
        "        output_before_softmax = disc(torch.tensor(x_lab))\n",
        "        test_batch_err = torch.mean(torch.ne(torch.argmax(output_before_softmax,dim=1),torch.tensor(labels)).float())\n",
        "        test_err += test_batch_err\n",
        "    for param,o_param in zip(disc.parameters(),original_params):\n",
        "        param.data.copy_(o_param)\n",
        "    test_err /= nr_batches_test\n",
        "    # report\n",
        "    print(\"Iteration %d, time = %ds, loss_lab = %.4f, loss_unl = %.4f, train err = %.4f, test err = %.4f\" % (epoch, time.time()-begin, loss_lab, loss_unl, train_err, test_err))\n",
        "    sys.stdout.flush()\n",
        "    gen.eval()\n",
        "    with torch.no_grad():\n",
        "        noise = torch.tensor(theano_rng.uniform(size=noise_dim))\n",
        "        sample_x = gen(torch.tensor(noise).float())\n",
        "        sample_x = sample_x.numpy()\n",
        "        img_bhwc = np.transpose(sample_x[:100,], (0, 2, 3, 1))\n",
        "        img_tile_o = img_tile(img_bhwc, aspect_ratio=1.0, border_color=1.0, stretch=True)\n",
        "        img = plot_img(img_tile_o, title='CIFAR10 samples')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P06exDb95BAb"
      },
      "source": [
        "    # Code to save params\n",
        "    #np.savez('disc_params.npz', *[p.get_value() for p in disc_params])\n",
        "    #np.savez('gen_params.npz', *[p.get_value() for p in gen_params])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}